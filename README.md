# Awesome-MLLM-SceneTextVQA

✨✨This repository includes MLLM works on the Scene-Text VQA tasks.

## Introduction 

- Scene-Text Visual Question Answering (Scene-Text VQA) is a task where models answer questions based on textual information within visual scenes (**images or videos**), requiring both scene text recognition and reasoning.
- With the rise of Multimodal Large Language Models (MLLMs), this task is crucial for enhancing scene text-aware multimodal understanding. It pushes MLLMs to **integrate visual-textual information** and **improve real-world applications** like document understanding and assistive technologies.

# Table of Contents

- **Awesome Papers**
  - [Image-based Multimodal Language Models](#image-based-multimodal-language-models)
  - [Video-based Multimodal Language Models](#video-based-multimodal-language-models)
  - [Others](#others)

- **Awesome Datasets**
  - [Image Datasets](#image-datasets)
  - [Video Datasets](#video-datasets)
  - [Synthetic Datasets](#synthetic-datasets)
  - [Domain-specific Datasets](#domain-specific-datasets)
  - [Others](#others-1)

---

## Awesome Papers

### Image-based Multimodal Language Models


| Title | Venue | Date | Code | Demo |
|-------|-------|------|------|------|
| [MiniCPM-V: A GPT-4V Level MLLM on Your Phone](https://arxiv.org/pdf/2408.01800) [![GitHub stars](https://img.shields.io/github/stars/OpenBMB/MiniCPM-V?style=social)](https://github.com/OpenBMB/MiniCPM-V)| arXiv | 2024-05-23 | [GitHub](https://github.com/OpenBMB/MiniCPM-V) | [Demo](https://huggingface.co/spaces/OpenBMB/MiniCPM-V)  |



### Video-based Multimodal Language Models
| Title | Venue | Date | Code | Demo |
|-------|-------|------|------|------|
| [MiniCPM-V 2.6: A GPT-4V Level MLLM for single image, multi-image and video understanding](https://arxiv.org/pdf/2408.01800) [![GitHub stars](https://img.shields.io/github/stars/OpenBMB/MiniCPM-V?style=social)](https://github.com/OpenBMB/MiniCPM-V)| arXiv | 2024-08-06 | [GitHub](https://github.com/OpenBMB/MiniCPM-V) | [Demo](http://120.92.209.146:8887/)  |
| [ShareGPT4Video: Improving Video Understanding and Generation with Better Captions](https://arxiv.org/pdf/2409.12191) [![GitHub stars](https://img.shields.io/github/stars/ShareGPT4Omni/ShareGPT4Video?style=social)](https://github.com/ShareGPT4Omni/ShareGPT4Video)| NeurIPS D&B track | 2024-10-01 | [GitHub](https://github.com/ShareGPT4Omni/ShareGPT4Video) | [Demo](https://huggingface.co/spaces/Lin-Chen/ShareCaptioner-Video)  
| [Qwen2-VL: Enhancing Vision-Language Model’s Perception of the World at Any Resolution](https://arxiv.org/pdf/2409.12191) [![GitHub stars](https://img.shields.io/github/stars/QwenLM/Qwen2-VL?style=social)](https://github.com/QwenLM/Qwen2-VL)| arXiv | 2024-10-03 | [GitHub](https://github.com/QwenLM/Qwen2-VL) | [Demo](https://huggingface.co/spaces/Qwen/Qwen2-VL)  |
| [Qwen2-VL: Enhancing Vision-Language Model’s Perception of the World at Any Resolution](https://arxiv.org/pdf/2409.12191) [![GitHub stars](https://img.shields.io/github/stars/QwenLM/Qwen2-VL?style=social)](https://github.com/QwenLM/Qwen2-VL)| arXiv | 2024-10-03 | [GitHub](https://github.com/QwenLM/Qwen2-VL) | [Demo](https://huggingface.co/spaces/Qwen/Qwen2-VL)  |




### Others

---

## Awesome Datasets

### Image Datasets

### Video Datasets


### Synthetic Datasets


### Domain-specific Datasets

### Others


