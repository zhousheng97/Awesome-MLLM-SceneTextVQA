# Awesome-MLLM-SceneTextVQA
## Introduction 

TextVQA is a fine-grained direction of the VQA task, which aims to read the text in the image and answer questions by reasoning about the text and visual content.

**Note**: In the TextVQA task, here are some of the reference materials in the process of my own research.

## Chanlleng 
ICDAR 2019 Robust Reading Challenge on Scene Text Visual Question Answering[[overview](https://rrc.cvc.uab.es/?ch=11)][[result](https://rrc.cvc.uab.es/?ch=11&com=evaluation&task=1)]

## Papers
### 2021
- ( **AAAI** )Simple is not Easy: A Simple Strong Baseline for TextVQA and TextCaps. [[paper](https://arxiv.org/abs/2012.05153)](**3-Att-Blok**)

### 2020
- ( **CVPR** )Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA. [[paper](https://arxiv.org/abs/1911.06258)][[code](https://github.com/facebookresearch/mmf)](**M4C**)
- ( **ACM MM** )Cascade Reasoning Network for Text-basedVisual Question Answering. [[paper](https://dl.acm.org/doi/abs/10.1145/3394171.3413924)][[code](https://github.com/guanghuixu/CRN_tvqa)](**CRN**)
- ( **CVPR** )Multi-Modal Graph Neural Network for Joint Reasoning on Vision and Scene Text. [[paper](https://arxiv.org/abs/2003.13962)][[code](https://github.com/ricolike/mmgnn_textvqa)](**MM-GNN**)
- ( **ECCV** )Spatially Aware Multimodal Transformers for TextVQA. [[paper](https://arxiv.org/abs/2007.12146)][[code](https://github.com/yashkant/sam-textvqa)](**SA-M4C**)
- ( **COLING** )Finding the Evidence: Localization-aware Answer Prediction for Text Visual Question Answering. [[paper](https://arxiv.org/abs/2010.02582)](**LaAP**)
- ( **CVPR** )On the General Value of Evidence, and Bilingual Scene-Text Visual Question Answering. [[paper](https://arxiv.org/abs/2002.10215)](**Dataset:EST-VQA/Model:QA R-CNN**)
- ( **arxiv** )Multimodal grid features and cell pointers for Scene Text Visual Question Answering. [[paper](https://arxiv.org/abs/2006.00923)]
- ( **Report** )Structured Multimodal Attentions for TextVQA. [[paper](https://arxiv.org/abs/2006.00753)](**SMA**)
- ( **Report** ) TAP: Text-Aware Pre-training for Text-VQA and Text-Caption. [[paper](https://arxiv.org/abs/2012.04638)](**TAP**)

### 2019
- ( **CVPR** )Towards VQA Models That Can Read. [[paper](https://arxiv.org/abs/1904.08920)][[code](https://github.com/facebookresearch/mmf)] -(**LoRRA/Dataset:TextVQA**)
- ( **ICCV** ) Scene Text Visual Question Answering. [[paper](https://arxiv.org/abs/1905.13648)] -(**Dataset:ST-VQA**)
- ( **ICCV** )From Strings to Things: Knowledge-enabled VQA Model that can Read and Reason.  [[paper](https://ieeexplore.ieee.org/document/9010987)](**K-VQA**)
- ( **ICDAR** )OCR-VQA: Visual Question Answering by Reading Text in Images.  [[paper](https://ieeexplore.ieee.org/document/8978122)] -(**Dataset:OCR-VQA**)
